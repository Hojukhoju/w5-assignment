{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yiqdVo0a0Zd8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OX8gnvl0cYR",
        "outputId": "9a504e90-738e-45db-ee35-486f6099d3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1s0ePvYi0rj-"
      },
      "outputs": [],
      "source": [
        "# Load your dataset\n",
        "def load_data(file_path):\n",
        "    file_path = \"/content/drive/MyDrive/stock_trading_data.csv\"\n",
        "    # Assuming the dataset has 'text' and 'label' columns\n",
        "    data = pd.read_csv(file_path)\n",
        "    texts = data['Date'].astype(str).tolist()  # Convert 'Date' to strings\n",
        "    labels = data['Low'].values  # Replace 'Low' with your target column if different\n",
        "    return texts, labels  # Return the text column as a list of strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KDp4mUT8481E"
      },
      "outputs": [],
      "source": [
        "# Normalize the labels if they are in a wide range\n",
        "def preprocess_data(texts, labels, vocab_size=10000, max_length=100):\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    labels_scaled = scaler.fit_transform(labels.reshape(-1, 1)).flatten()  # Normalize between 0 and 1\n",
        "\n",
        "    # Original return statement:\n",
        "    # return padded_sequences, labels_scaled, tokenizer, sequences, scaler\n",
        "\n",
        "    # Modified to return 4 values as expected:\n",
        "    return padded_sequences, labels_scaled, tokenizer, sequences # Removed 'scaler' from return values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5ZOqiVLU59EQ"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "file_path = \"/content/drive/MyDrive/stock_trading_data.csv\"  # Replace with your dataset path\n",
        "texts, labels = load_data(file_path)\n",
        "\n",
        "# Preprocess the data and store tokenizer\n",
        "padded_sequences, labels_scaled, tokenizer, sequences = preprocess_data(texts, labels)\n",
        "\n",
        "# Access the vocabulary via tokenizer.word_index\n",
        "vocab = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "F4U_6OGz03cy"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and testing\n",
        "x_train_texts, x_test_texts, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "vocab_size = 10000  # Adjust the vocabulary size\n",
        "max_length = 100    # Adjust sequence length if needed\n",
        "\n",
        "# preprocess_data function returns 4 values: padded_sequences, labels_scaled, tokenizer, sequences\n",
        "x_train, y_train, tokenizer, sequences = preprocess_data(x_train_texts, y_train, vocab_size, max_length)\n",
        "x_test, y_test, _, _ = preprocess_data(x_test_texts, y_test, vocab_size, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OCh5Pel12YH",
        "outputId": "b58ea6ef-2f1e-4369-cad6-fd2fbeff6b90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0, 1, 2, 3, 4, 5]]), array([[1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X = np.array(range(0, 6))  # Creates an array [0, 1, 2, 3, 4, 5]\n",
        "X = X.reshape(1, 6)  # 1 sample with 6 features\n",
        "y = np.array([1])  # Single label\n",
        "y= y.reshape(-1,1)\n",
        "X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtijukZ71EAF",
        "outputId": "c67a3807-e764-48a0-959a-671930cc2c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (top 6): {'<OOV>': 1, '2019': 2, '2020': 3, '2018': 4, '2017': 5, '06': 6, '03': 7, '11': 8, '12': 9, '10': 10, '2016': 11, '01': 12, '07': 13, '08': 14, '05': 15, '04': 16, '09': 17, '02': 18, '2021': 19, '26': 20, '13': 21, '21': 22, '28': 23, '19': 24, '23': 25, '15': 26, '22': 27, '18': 28, '14': 29, '24': 30, '16': 31, '29': 32, '30': 33, '25': 34, '27': 35, '20': 36, '17': 37, '31': 38}\n"
          ]
        }
      ],
      "source": [
        "vocab_size = 6  # Consider only the top 4 most frequent words\n",
        "max_length = 10  # Adjust sequence length if needed\n",
        "\n",
        "x_train, y_train, tokenizer, sequences = preprocess_data(x_train_texts, y_train, vocab_size, max_length)\n",
        "x_test, y_test, _, _ = preprocess_data(x_test_texts, y_test, vocab_size, max_length)\n",
        "\n",
        "# Access the vocabulary\n",
        "vocab = tokenizer.word_index\n",
        "print(f\"Vocabulary (top {vocab_size}): {vocab}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X_qSUHx1SNO",
        "outputId": "358511d1-b19f-4252-fa4f-36d1def3df1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Build the RNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length),  # Embedding layer\n",
        "    SimpleRNN(units=32, activation='tanh', return_sequences=False, name=\"Simple_RNN\"),  # RNN layer\n",
        "    Dense(units=1, activation='sigmoid', name=\"Output_Layer\")  # Output layer for binary classification\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QHDHkn701VWB"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2_SCP-s8Yxx",
        "outputId": "5062a300-8420-4cfe-955f-5fc9fcc9e058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.0021 - loss: 0.6262\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c12ecb46a70>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Assuming 'x_train', 'y_train', 'x_test', 'y_test' are defined as per ipython-input-133-9abc114218f5\n",
        "model.fit(x_train, y_train)  # Use x_train and y_train instead of sequences and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaF8JcWe1bZ1",
        "outputId": "0f1b8f94-25cc-45d2-b1a6-46bd7ea7e094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.0013 - loss: 0.5925 - val_accuracy: 0.0000e+00 - val_loss: 0.5908\n",
            "Epoch 2/5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0016 - loss: 0.5852 - val_accuracy: 0.0000e+00 - val_loss: 0.5899\n",
            "Epoch 3/5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0016 - loss: 0.5930 - val_accuracy: 0.0000e+00 - val_loss: 0.5900\n",
            "Epoch 4/5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0016 - loss: 0.5878 - val_accuracy: 0.0000e+00 - val_loss: 0.5898\n",
            "Epoch 5/5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0016 - loss: 0.5846 - val_accuracy: 0.0000e+00 - val_loss: 0.5900\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9eCJjxW1f3G",
        "outputId": "94726ad8-f0f2-430c-9bea-107d04fede4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0014 - loss: 0.6363     \n",
            "Test Loss: 0.6324719786643982, Test Accuracy: 0.003968254197388887\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}